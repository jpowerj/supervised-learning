---
title: "Supervised Learning with Parametric Models"
author: "Jeff Jacobs"
format:
  revealjs:
    smaller: true
    scrollable: true
    footer: "Supervised Learning with Parametric Models"
    css: gtown.css
    df-print: kable
    theme: [default, jj_custom.scss]
editor: visual
toc: false
number-sections: false
---

```{r setup, include=FALSE}
set.seed(1948)
library(knitr)
library(ggplot2)
library(dplyr)
library(ggforce)
library(patchwork)
cbPalette <- c("#E69F00", "#56B4E9", "#009E73", "#F0E442", "#0072B2", "#D55E00", "#CC79A7")
options(ggplot2.discrete.colour= cbPalette)
global_theme <- ggplot2::theme_classic() + ggplot2::theme(
  plot.title = element_text(hjust = 0.5, size = 18),
  axis.title = element_text(size = 14),
  axis.text = element_text(size = 12),
  legend.title = element_text(size = 14, hjust = 0.5),
  legend.text = element_text(size = 12),
  legend.box.background = element_rect(colour = "black")
)
knitr::opts_chunk$set(fig.align="center")
g_pointsize <- 6
# Global functions
get_rss <- function(model) { return(sum(summary(model)$residuals^2)) }
```

## Supervised vs. Unsupervised Learning

::: columns
::: {.column width="50%"}
-   **Supervised** Learning: You want the computer to learn the pattern of how *you* are classifying^[*Whether standard **classification** (sorting each observation into a bin) or **regression** (assigning a real number to each observation)*] observations in a dataset
    -   Discovering the relationship between *properties* of data and *outcomes*
    -   Example (*Binary Classification*): I look at homes on Zillow, saving those I like to folder A and don't like to folder B
    -   Example (*Regression*): I assign a rating of 0-100 to each home
    -   In both cases: I ask the computer to *learn* my preferences (how I classify/assign ratings to homes)
:::

::: {.column width="50%"}
-   **Unsupervised** Learning: You want the computer to *find* patterns in a dataset, without any classification info
    -   Typically: grouping or clustering observations based on shared properties
    -   Example (*Clustering*): I save all the used car listings I can find, and ask the computer to "find a pattern" in this data, by clustering similar cars together
    -   ["Exploratory" Data Analysis](https://www.ibm.com/topics/exploratory-data-analysis)
:::
:::

## Dataset Structures

::: columns
::: {.column width="50%"}

-   **Supervised** Learning: Dataset has both [*explanatory* variables ("features")]{.colA} and [*response* variables ("labels")]{.colB}

```{r}
sup_data <- tibble::tribble(
  ~home_id, ~sqft, ~bedrooms, ~rating,
  0, 1000, 1, "Disliked",
  1, 2000, 2, "Liked",
  2, 2500, 1, "Liked",
  3, 1500, 2, "Disliked",
  4, 2200, 1, "Liked"
)
```

::: suptable
```{r}
sup_data
```
:::
:::

::: {.column width="50%"}

-   **Unsupervised** Learning: Dataset has only [*explanatory* variables]{.colA}

```{r}
unsup_data <- tibble::tribble(
  ~home_id, ~sqft, ~bedrooms,
  0, 1000, 1,
  1, 2000, 2,
  2, 2500, 1,
  3, 1500, 2,
  4, 2200, 1
)
```

::: unsuptable
```{r}
unsup_data
```
:::
:::
:::


## Dataset Structures: Visualized

::: columns
::: {.column width="50%"}
```{r, fig.width=5, fig.height=5.5}
ggplot(sup_data, aes(x=sqft, y=bedrooms, color=rating)) + 
  geom_point(size=g_pointsize) +
  labs(
    title = "Supervised Data: House Listings",
    x = "Square Footage",
    y = "Number of Bedrooms",
    color = "Outcome"
  ) +
  expand_limits(x=c(800,2700), y=c(0.8,2.2)) +
  global_theme
```
:::

::: {.column width="50%"}
```{r, fig.width=5, fig.height=5.5}
# To force a legend
unsup_grouped <- unsup_data %>% mutate(big=bedrooms > 1)
unsup_grouped[['big']] <- factor(unsup_grouped[['big']], labels=c("?1","?2"))
ggplot(unsup_grouped, aes(x=sqft, y=bedrooms, fill=big)) + 
  geom_point(size=g_pointsize) +
  labs(
    x = "Square Footage",
    y = "Number of Bedrooms",
    fill = "?"
  ) +
  global_theme +
  expand_limits(x=c(800,2700), y=c(0.8,2.2)) +
  ggtitle("Unsupervised Data: House Listings") +
  theme(legend.background = element_rect(fill="white", color="white"), legend.box.background = element_rect(fill="white"), legend.text = element_text(color="white"), legend.title = element_text(color="white"), legend.position = "right") +
  scale_fill_discrete(labels=c("?","?")) +
  #scale_color_discrete(values=c("white","white"))
  scale_color_manual(name=NULL, values=c("white","white")) +
  #scale_color_manual(values=c("?1"="white","?2"="white"))
  guides(fill = guide_legend(override.aes = list(shape = NA)))
```
:::
:::

## Different Goals

::: columns
::: {.column width="50%"}
```{r, fig.width=5, fig.height=5.5}
ggplot(sup_data, aes(x=sqft, y=bedrooms, color=rating)) + 
  geom_point(size=g_pointsize) +
  labs(
    title = "Supervised Data: House Listings",
    x = "Square Footage",
    y = "Number of Bedrooms",
    color = "Outcome"
  ) +
  global_theme +
  expand_limits(x=c(800,2700), y=c(0.8,2.2)) +
  geom_vline(xintercept = 1750, linetype="dashed", color = "black", size=1) +
  annotate('rect', xmin=-Inf, xmax=1750, ymin=-Inf, ymax=Inf, alpha=.2, fill=cbPalette[1]) +
  annotate('rect', xmin=1750, xmax=Inf, ymin=-Inf, ymax=Inf, alpha=.2, fill=cbPalette[2])
  #geom_rect(aes(xmin=-Inf, xmax=Inf, ymin=0, ymax=Inf, alpha=.2, fill='red'))
```
:::

::: {.column width="50%"}
```{r, fig.width=5, fig.height=5.5}
ggplot(unsup_grouped, aes(x=sqft, y=bedrooms)) +
  #scale_color_brewer(palette = "PuOr") +
  geom_mark_ellipse(expand=0.1, aes(fill=big), size = 1) +
  geom_point(size=g_pointsize) +
  labs(
    x = "Square Footage",
    y = "Number of Bedrooms",
    fill = "?"
  ) +
  global_theme +
  ggtitle("Unsupervised Data: House Listings") +
  #theme(legend.position = "none") +
  #theme(legend.title = text_element("?"))
  expand_limits(x=c(800,2700), y=c(0.8,2.2)) +
  scale_fill_manual(values=c(cbPalette[3],cbPalette[4]), labels=c("?","?"))
  #scale_fill_manual(labels=c("?","?"))
```
:::
:::

## The "Learning" in Machine Learning

-   Given these datasets, how do we learn the patterns?
-   NaÃ¯ve idea: Try random lines (each forming a **decision boundary**), pick "best" one

```{r, include=FALSE}
x_min <- 0
x_max <- 3000
y_min <- -1
y_max <- 3
rand_y0 <- runif(50, min=y_min, max=y_max)
rand_y1 <- runif(50, min=y_min, max=y_max)
rand_slope <- (rand_y1 - rand_y0)/(x_max - x_min)
rand_intercept <- rand_y0
rand_lines <- tibble::tibble(id=1:50, slope=rand_slope, intercept=rand_intercept)
ggplot() +
  geom_abline(data=rand_lines, aes(slope=slope, intercept=intercept)) +
  xlim(0,3000) +
  ylim(0,2)
```

::: r-stack
::: fragment
```{r, fig.height=4.75}
ggplot(sup_data, aes(x=sqft, y=bedrooms, color=rating)) + 
  geom_point(size=g_pointsize) +
  labs(
    title = "The Like vs. Dislike Boundary: 50 Guesses",
    x = "Square Footage",
    y = "Number of Bedrooms",
    color = "Outcome"
  ) +
  global_theme +
  expand_limits(x=c(800,2700), y=c(0.8,2.2)) +
  geom_abline(data=rand_lines, aes(slope=slope, intercept=intercept), linetype="dashed")
```
:::
:::

## What Makes a "Good"/"Best" Guess? ðŸ¤”

- What's your intuition? How about accuracy...

```{r}
#base_plot <- ggplot(sup_data, aes(x=sqft, y=bedrooms, color=liked)) + 
#  geom_point(size=g_pointsize) +
#  labs(
#    x = "Square Footage",
#    y = "Number of Bedrooms",
#    color = "Liked?"
#  ) +
#  global_theme +
#  expand_limits(x=c(800,2700), y=c(0.8,2.2))

line_data <- tibble::tribble(
  ~id, ~slope, ~intercept,
  0, 0, 0.75,
  1, 0.00065, 0.5
)
data_range <- 800:2700
ribbon_range <- c(-Inf,data_range,Inf)
f1 <- function(x) { return(0*x + 0.75) }
f1_data <- tibble::tibble(line_x=ribbon_range,line_y=c(f1(700),f1(data_range),f1(3100)))
g1_plot <- ggplot(data=(line_data %>% filter(id==0))) +
  geom_abline(aes(slope=slope, intercept=intercept), linetype="dashed", size=1) +
  ggtitle("Guess 1: 60% Accuracy") +
  geom_point(data=sup_data, aes(x=sqft, y=bedrooms, color=rating), size=g_pointsize) +
  geom_ribbon(data=f1_data, aes(x=line_x, ymin=-Inf, ymax=line_y, fill="Disliked"), alpha=0.2) +
  geom_ribbon(data=f1_data, aes(x=line_x, ymin=line_y, ymax=Inf, fill="Liked"), alpha=0.2) +
  labs(
    x = "Square Footage",
    y = "Number of Bedrooms",
    color = "True Label"
  ) +
  global_theme +
  expand_limits(x=data_range, y=c(0.8,2.2)) +
  scale_fill_manual(values=c("Liked"=cbPalette[2],"Disliked"=cbPalette[1]), name="Guess")

f2 <- function(x) { return(0.00065*x + 0.5) }
f2_data <- tibble::tibble(line_x=ribbon_range,line_y=c(f2(710),f2(data_range),f2(Inf)))
g2_plot <- ggplot(data=(line_data %>% filter(id==1))) +
  geom_abline(aes(slope=slope, intercept=intercept), linetype="dashed", size=1) +
  ggtitle("Guess 2: 60% Accuracy") +
  geom_point(data=sup_data, aes(x=sqft, y=bedrooms, color=rating), size=g_pointsize) +
  labs(
    x = "Square Footage",
    y = "Number of Bedrooms",
    color = "True Label"
  ) +
  geom_ribbon(data=f2_data, aes(x=line_x, ymin=-Inf, ymax=line_y, fill="Liked"), alpha=0.2) +
  geom_ribbon(data=f2_data, aes(x=line_x, ymin=line_y, ymax=Inf, fill="Disliked"), alpha=0.2) +
  global_theme +
  expand_limits(x=data_range, y=c(0.8,2.2)) +
  scale_fill_manual(values=c("Liked"=cbPalette[2],"Disliked"=cbPalette[1]), name="Guess")

g1_plot + g2_plot
```
So... what's wrong here?

## What's Wrong with Accuracy?

```{r}
#base_plot <- ggplot(sup_data, aes(x=sqft, y=bedrooms, color=liked)) + 
#  geom_point(size=g_pointsize) +
#  labs(
#    x = "Square Footage",
#    y = "Number of Bedrooms",
#    color = "Liked?"
#  ) +
#  global_theme +
#  expand_limits(x=c(800,2700), y=c(0.8,2.2))

gen_homes <- function(n) {
  rand_sqft <- runif(n, min=2000, max=3000)
  rand_bedrooms <- sample(c(1,2), size=n, prob=c(0.5,0.5), replace=TRUE)
  rand_ids <- 1:n
  rand_rating <- "Liked"
  rand_tibble <- tibble::tibble(home_id=rand_ids, sqft=rand_sqft, bedrooms=rand_bedrooms, rating=rand_rating)
  return(rand_tibble)
}
fake_homes <- gen_homes(18)
fake_sup_data <- dplyr::bind_rows(sup_data, fake_homes)
line_data <- tibble::tribble(
  ~id, ~slope, ~intercept,
  0, 0, 0.75,
  1, 0.00065, 0.5
)
f1 <- function(x) { return(0*x + 0.75) }
f2 <- function(x) { return(0.00065*x + 0.5) }
# And check accuracy
fake_sup_data <- fake_sup_data %>% mutate(boundary1=f1(sqft)) %>% mutate(guessDislike1 = bedrooms < boundary1) %>% mutate(correct1 = ((rating=="Disliked") & (guessDislike1)) | (rating=="Liked") & (!guessDislike1))
fake_sup_data <- fake_sup_data %>% mutate(boundary2=f2(sqft)) %>% mutate(guessDislike2 = bedrooms > boundary2) %>% mutate(correct2 = ((rating=="Disliked") & (guessDislike2)) | (rating=="Liked") & (!guessDislike2))

data_range <- 800:2700
ribbon_range <- c(-Inf,data_range,Inf)

f1_data <- tibble::tibble(line_x=ribbon_range,line_y=c(f1(700),f1(data_range),f1(3200)))
g1_plot <- ggplot(data=(line_data %>% filter(id==0))) +
  geom_abline(aes(slope=slope, intercept=intercept), linetype="dashed", size=1) +
  ggtitle("Guess 1: 91.3% Accuracy") +
  geom_point(data=fake_sup_data, aes(x=sqft, y=bedrooms, fill=rating, color=rating, shape=factor(correct1, levels=c(TRUE,FALSE))), size=g_pointsize) +
  scale_shape_manual(values=c(24, 25)) +
  geom_ribbon(data=f1_data, aes(x=line_x, ymin=-Inf, ymax=line_y, fill="Disliked"), alpha=0.2) +
  geom_ribbon(data=f1_data, aes(x=line_x, ymin=line_y, ymax=Inf, fill="Liked"), alpha=0.2) +
  labs(
    x = "Square Footage",
    y = "Number of Bedrooms",
    color = "True Label",
    shape = "Correct Guess"
  ) +
  global_theme +
  expand_limits(x=data_range, y=c(0.8,2.2)) +
  scale_fill_manual(values=c("Liked"=cbPalette[2],"Disliked"=cbPalette[1]), name="Guess")


f2_data <- tibble::tibble(line_x=ribbon_range,line_y=c(f2(700),f2(data_range),f2(3100)))
g2_plot <- ggplot(data=(line_data %>% filter(id==1))) +
  geom_abline(aes(slope=slope, intercept=intercept), linetype="dashed", size=1) +
  ggtitle("Guess 2: 73.9% Accuracy") +
  geom_point(data=fake_sup_data, aes(x=sqft, y=bedrooms, fill=rating, color=rating, shape=factor(correct2, levels=c(TRUE,FALSE))), size=g_pointsize) +
  scale_shape_manual(values=c(24, 25)) +
  labs(
    x = "Square Footage",
    y = "Number of Bedrooms",
    color = "True Label",
    shape = "Correct Guess"
  ) +
  geom_ribbon(data=f2_data, aes(x=line_x, ymin=-Inf, ymax=line_y, fill="Liked"), alpha=0.2) +
  geom_ribbon(data=f2_data, aes(x=line_x, ymin=line_y, ymax=Inf, fill="Disliked"), alpha=0.2) +
  global_theme +
  expand_limits(x=data_range, y=c(0.8,2.2)) +
  scale_fill_manual(values=c("Liked"=cbPalette[2],"Disliked"=cbPalette[1]), name="Guess")

g1_plot + g2_plot
```

## Measuring Errors: F1 Score

- How can we reward guesses which best *discriminate* between classes?

$$
\begin{align*}
\mathsf{Precision} &= \frac{\# \text{true positives}}{\# \text{predicted positive}} = \frac{tp}{tp+fp} \\[1.5em]
\mathsf{Recall} &= \frac{\# \text{true positives}}{\# \text{positives in data}} = \frac{tp}{tp+fn} \\[1.5em]
F_1 &= 2\frac{\mathsf{Precision} \cdot \mathsf{Recall}}{\mathsf{Precision} + \mathsf{Recall}} = \mathsf{HMean}(\mathsf{Precision}, \mathsf{Recall})
\end{align*}
$$

- How does this fix the issue with accuracy?

::: {.aside}
\f* $\mathsf{HMean}$ is the **harmonic mean**, an alternative to the standard (arithmetic) mean that penalizes greater "gaps" between precision and recall: if precision is 0 and recall is 1, for example, their *arithmetic* mean is 0.5 while their harmonic mean is 0. For the curious: given numbers $X = \{x_1, \ldots, x_n\}$, $\mathsf{HMean}(X) = \frac{n}{\sum_{i=1}^nx_i^{-1}}$
:::

## Measuring Errors: The Loss Function

- What about the regression case?
    - We can no longer just say "true prediction good, false prediction bad"
- We have to quantify **how bad** the guess is!
    - Then we can scale the penalty accordingly: $penalty \propto badness$
- Some common loss functions
$$
\mathcal{L}(y_{pred}, y_{obs}) = (y_{pred} - y_{obs})^2
$$

## Calculus Rears its Ugly Head

- Neural networks use **gradient descent** and **backpropagation** to improve their predictions given a particular loss function.
- Can we just use the $F_1$ score?
$$
\frac{\partial F_1(weights)}{\partial weights} = \ldots \; ? \; \ldots ðŸ’€
$$

## Quantifying Discrete Loss

- We can quantify a differentiable **discrete** loss by asking the algorithm *how confident* it is
    - Closer to 0 $\implies$ more confident that the true label is 0
    - Closer to 1 $\implies$ more confident that the true label is 1
$$
\mathcal{L}_{CE}(y_{pred}, y_{obs}) = -(y_{obs}\log(y_{pred}) + (1-y_{obs})\log(1-y_{pred}))
$$
```{r}
y_pred <- seq(from = 0, to = 1, by = 0.001)
compute_ce <- function(y_p, y_o) { return(-(y_o * log(y_p) + (1-y_o)*log(1-y_p))) }
ce0 <- compute_ce(y_pred, 0)
ce1 <- compute_ce(y_pred, 1)
ce0_data <- tibble::tibble(y_pred=y_pred, y_obs=0, ce=ce0)
ce1_data <- tibble::tibble(y_pred=y_pred, y_obs=1, ce=ce1)
ce_data <- dplyr::bind_rows(ce0_data, ce1_data)
ggplot(ce_data, aes(x=y_pred, y=ce, color=factor(y_obs))) +
  geom_line(linewidth=1) +
  labs(
    title="Binary Cross-Entropy Loss",
    x = "Predicted Value",
    y = "Loss",
    color = "Actual Value"
  ) +
  global_theme +
  ylim(0,6)
```

## Loss Function $\implies$ Ready to Learn!

- Once we've chosen a loss function, the learning algorithm has what it needs to proceed with the actual *learning*
- Notation: Bundle all the model's parameters together into $\theta$
- The goal:
$$
\min_{\theta} \mathcal{L}(y_{obs}, y_{pred}(\theta))
$$

- How do we solve this?

## Calculus Strikes Again

- tldr: The **slope** of a function tells us how to get to a minimum (why *a* minimum rather than *the* minimum?)
- Derivative (gradient) = "direction of sharpest decrease"
- Think of **hill climbing**! Let $\ell_t \in L$ be your location at time $t$, and $Alt(\ell)$ be the altitude at a location $\ell$
- Gradient descent for $\ell^* = \max_{\ell \in L} Alt(\ell)$:
$$
\ell_{t+1} = \ell_t + \gamma\nabla Alt(\ell_t),\ t\geq 0.
$$
- While top of mountain = good, **Loss** is bad: we want to find the **bottom** of the "loss crater"
    - $\implies$ we do the opposite: subtract $\gamma\nabla Alt(\ell_t)$

## Good and Bad News

::: columns
::: {.column width="50%"}

* Universal Approximation Theorem
* Neural networks can represent any function mapping one Euclidean space to another
* (Neural Turing Machines:)

<center>
![](NTM_Schematic.png){width="50%"}
</center>

:::

::: {.column width="50%}

* Weierstrass Approximation Theorem
* (Polynomials could already represent any function)

$$
f \in C([a,b],[a,b])
$$
$$
\implies \forall \epsilon > 0, \exists p \in \mathbb{R}[x] :
$$
$$
\forall x \in [a, b] \; \left|f(x) âˆ’ p(x)\right| < \epsilon
$$

* Implications for machine learning?

:::

:::

## So What's the Issue?

::: columns
::: {.column width="50%}

```{r}
x <- seq(from = 0, to = 1, by = 0.1)
n <- length(x)
eps <- rnorm(n, 0, 0.04)
y <- x + eps
# But make one big outlier
midpoint <- ceiling((3/4)*n)
y[midpoint] <- 0
of_data <- tibble::tibble(x=x, y=y)
# Linear model
lin_model <- lm(y ~ x)
# But now polynomial regression
poly_model <- lm(y ~ poly(x, degree = 10, raw=TRUE))
#summary(model)
```

```{r,fig.width=3.666,fig.height=2.5,fig.align="left"}
ggplot(of_data, aes(x=x, y=y)) +
  geom_point(size=g_pointsize/2) +
  labs(
    title = "Training Data",
    color = "Model"
  ) +
  global_theme
#  scale_color_manual(name="color", values=c("a","b","c"))
```

```{r,fig.width=5,fig.height=2.5}
ggplot(of_data, aes(x=x, y=y)) +
  geom_point(size=g_pointsize/2) +
  geom_abline(aes(intercept=0, slope=1, color="Linear"), linewidth=1, show.legend = FALSE) +
  stat_smooth(method = "lm",
              formula = y ~ poly(x, 10, raw=TRUE),
              se = FALSE, aes(color="Polynomial")) +
  labs(
    title = "A Perfect Model?",
    color = "Model"
  ) +
  global_theme
#  scale_color_manual(name="color", values=c("a","b","c"))
```
:::
::: {.column width="50%"}

- Higher $R^2$ = Better Model? Lower $RSS$?
- Linear Model:

```{r,echo=TRUE}
summary(lin_model)$r.squared
get_rss(lin_model)
```
- Polynomial Model:

```{r,echo=TRUE}
summary(poly_model)$r.squared
get_rss(poly_model)
```

:::
:::

## Generalization

- Training Accuracy: How well does it fit the data we can see?
- Test Accuracy: How well does it **generalize** to future data?

::: columns
::: {.column width="50%}
```{r}
# Data setup
x_test <- seq(from = 0, to = 1, by = 0.1)
n_test <- length(x_test)
eps_test <- rnorm(n_test, 0, 0.04)
y_test <- x_test + eps_test
of_data_test <- tibble::tibble(x=x_test, y=y_test)
lin_y_pred_test <- predict(lin_model, as.data.frame(x_test))
#lin_y_pred_test
lin_resids_test <- y_test - lin_y_pred_test
#lin_resids_test
lin_rss_test <- sum(lin_resids_test^2)
#lin_rss_test
# Lin R2 = 1 - RSS/TSS
tss_test <- sum((y_test - mean(y_test))^2)
lin_r2_test <- 1 - (lin_rss_test / tss_test)
#lin_r2_test
# Now the poly model
poly_y_pred_test <- predict(poly_model, as.data.frame(x_test))
poly_resids_test <- y_test - poly_y_pred_test
poly_rss_test <- sum(poly_resids_test^2)
#poly_rss_test
# RSS
poly_r2_test <- 1 - (poly_rss_test / tss_test)
#poly_r2_test
```

```{r,fig.width=5.5,fig.height=4}
ggplot(of_data, aes(x=x, y=y)) +
  stat_smooth(method = "lm",
              formula = y ~ poly(x, 10, raw = TRUE),
              se = FALSE, aes(color="Polynomial")) +
  global_theme +
  geom_point(data=of_data_test, aes(x=x_test, y=y_test), size=g_pointsize/2) +
  geom_abline(aes(intercept=0, slope=1, color="Linear"), linewidth=1, show.legend = FALSE) +
  labs(
    title = "Getting a Perfect Fit: Unseen Test Data",
    color = "Model"
  ) +
  global_theme
```
:::
::: {.column width="50%"}
- Linear Model:

```{r,echo=TRUE}
lin_r2_test
lin_rss_test
```

- Polynomial Model:
```{r,echo=TRUE}
poly_r2_test
poly_rss_test
```
:::
:::

## How to Avoid Overfitting?

- The gist: penalize model **complexity**

Original optimization:
$$
\min_{params} \mathcal{L}(y_{obs}, y_{pred}(params))
$$

New optimization:
$$
\min_{\theta} \left[ \mathcal{L}(y_{obs}, y_{pred}(\theta)) + \mathsf{Complexity}(\theta) \right]
$$

- So how do we measure, and penalize, "complexity"?

## Regularization: Measuring and Penalizing Complexity

- In the case of polynomials, fairly simple complexity measure: *degree* of polynomial

$$
\mathsf{Complexity}(y_{pred} = \beta_0 + \beta_1 x + \beta_2 x^2 + \beta_3 x^3) > \mathsf{Complexity}(y_{pred} = \beta_0 + \beta_1 x)
$$

- In general machine learning, however, we might not be working with polynomials
- In neural networks, for example, we sometimes toss in millions of features and ask the algorithm to "just figure it out"
- The gist, in the general case, is thus: try to "amplify" the most important features and shrink the rest, so that

$$
\mathsf{Complexity} \propto \frac{|\text{AmplifiedFeatures}|}{|\text{ShrunkFeatures}|}
$$

## LASSO Regression

- Many ways to translate this intuition into math!
- In several fields, however (econ, biostatistics), LASSO^[**L**east **A**bsolute **S**hrinkage and **S**election **O**perator] is most popular:

$$
\beta^* = \min _{\beta \in \mathbb {R} ^{p}}\left\{{\frac {1}{N}}\left\|y-X\beta \right\|_{2}^{2}+\lambda \|\beta \|_{1}\right\}
$$

- Why does this work to penalize complexity?
- What does the parameter $\lambda$ do?

## Training vs. Test Data

```{dot}
//| fig-size: 3
graph grid
{
    graph [
        overlap=true
    ]
    nodesep=0.0
    ranksep=0.0
    rankdir="TB"
    node [
        style="filled",
        color=black,
        fillcolor=lightblue,
        shape=box
    ]

	// uncomment to hide the grid
	edge [style=invis]
	
	subgraph cluster_01 {
	    label="Training Set (80%)"
	N1[label="20%"] N2[label="20%"] N3[label="20%"] N4[label="20%"]
	}
	subgraph cluster_02 {
	    label="Test Set (20%)"
	N5[label="20%",fillcolor=orange]
	}
}
```

## Cross-Validation

- The idea that **good models generalize well** is crucial!
  - What if we could leverage this insight to optimize **over our training data**?
  - The key: **Validation Sets**
  
<svg width="960" height="400" viewBox="0.00 0.00 491.00 142.00" xmlns="http://www.w3.org/2000/svg" xlink="http://www.w3.org/1999/xlink" style=" display: block; margin: auto auto auto auto">
<g id="graph0" class="graph" transform="scale(1 1) rotate(0) translate(4 138)">
<title>
grid
</title>
<polygon fill="white" stroke="transparent" points="-4,4 -4,-138 487,-138 487,4 -4,4"></polygon> <g id="clust1" class="cluster">
<title>
cluster_01
</title>
<polygon fill="none" stroke="black" points="8,-8 8,-126 376,-126 376,-8 8,-8"></polygon> <text text-anchor="middle" x="192" y="-109.4" font-family="Times,serif" font-size="14.00">Training Set (80%)</text> </g> <g id="clust3" class="cluster">
<title>
cluster_03
</title>
<polygon fill="none" stroke="black" points="226,-16 226,-93 368,-93 368,-16 226,-16"></polygon> <text text-anchor="middle" x="297" y="-76.4" font-family="Times,serif" font-size="14.00">Validation Fold (25%)</text> </g> <g id="clust2" class="cluster">
<title>
cluster_02
</title>
<polygon fill="none" stroke="black" points="16,-16 16,-93 210,-93 210,-16 16,-16"></polygon> <text text-anchor="middle" x="113" y="-76.4" font-family="Times,serif" font-size="14.00">Training Fold (75%)</text> </g> <g id="clust4" class="cluster">
<title>
cluster_04
</title>
<polygon fill="none" stroke="black" points="384,-16 384,-93 483,-93 483,-16 384,-16"></polygon> <text text-anchor="middle" x="433.5" y="-76.4" font-family="Times,serif" font-size="14.00">Test Set (20%)</text> </g> <!-- A1 --> <g id="node1" class="node">
<title>
A1
</title>
<polygon fill="lightblue" stroke="black" points="78,-60 24,-60 24,-24 78,-24 78,-60"></polygon> <text text-anchor="middle" x="51" y="-37.8" font-family="Times,serif" font-size="14.00">20%</text> </g> <!-- A2 --> <g id="node2" class="node">
<title>
A2
</title>
<polygon fill="lightblue" stroke="black" points="140,-60 86,-60 86,-24 140,-24 140,-60"></polygon> <text text-anchor="middle" x="113" y="-37.8" font-family="Times,serif" font-size="14.00">20%</text> </g> <!-- A1&#45;&#45;A2 --> <!-- A3 --> <g id="node3" class="node">
<title>
A3
</title>
<polygon fill="lightblue" stroke="black" points="202,-60 148,-60 148,-24 202,-24 202,-60"></polygon> <text text-anchor="middle" x="175" y="-37.8" font-family="Times,serif" font-size="14.00">20%</text> </g> <!-- A2&#45;&#45;A3 --> <!-- B1 --> <g id="node4" class="node">
<title>
B1
</title>
<polygon fill="lightgreen" stroke="black" points="324,-60 270,-60 270,-24 324,-24 324,-60"></polygon> <text text-anchor="middle" x="297" y="-37.8" font-family="Times,serif" font-size="14.00">20%</text> </g> <!-- A3&#45;&#45;B1 --> <!-- C1 --> <g id="node5" class="node">
<title>
C1
</title>
<polygon fill="orange" stroke="black" points="460,-60 406,-60 406,-24 460,-24 460,-60"></polygon> <text text-anchor="middle" x="433" y="-37.8" font-family="Times,serif" font-size="14.00">20%</text> </g> <!-- B1&#45;&#45;C1 --> </g>
</svg>

## Hyperparameter Tuning

## Prediction

## Lab

## Q & A
